{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e94b70",
   "metadata": {},
   "source": [
    " # ğŸš€ Final Training: ResNet50 + Augmented Data\n",
    " ×–×”×• ×”×¡×§×¨×™×¤×˜ ×”×¡×•×¤×™ ×œ××™××•×Ÿ ×”××•×“×œ.\n",
    " ×”×•× ×›×•×œ×œ:\n",
    " 1. ××•×“×œ ×× ×¦×—: ResNet50 Dual-Head.\n",
    " 2. ×¤×•× ×§×¦×™×•×ª ×”×¤×¡×“: BCE (×œ××¤×”) + MSE (×œ×•×•×§×˜×•×¨×™×).\n",
    " 3. ×“××˜×” ××œ×: ×›×œ ×ª××•× ×•×ª ×”××™××•×Ÿ.\n",
    " 4. ××•×’×× ×˜×¦×™×•×ª ×—×›××•×ª: ×”×™×¤×•×š (×›×•×œ×œ ×ª×™×§×•×Ÿ ×•×§×˜×•×¨×™×), ×¦×‘×¢, ×˜×©×˜×•×© ×•×¡×™×‘×•×‘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e5a0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Training Setup ---\n",
      "Device: cuda\n",
      "Backbone: ResNet50\n",
      "Augmentations: ON (Flip, Color, Rotate +/-5, Blur)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- ×”×’×“×¨×•×ª ×¡×•×¤×™×•×ª ---\n",
    "BATCH_SIZE = 4          # ResNet50 ×”×•× ×›×‘×“, × ×•×¨×™×“ ×œ-4 ×›×“×™ ×œ× ×œ×¤×•×¦×¥ ×–×™×›×¨×•×Ÿ\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 20         # ××™××•×Ÿ ××¨×•×š ×•××œ×\n",
    "DATA_DIR = Path(\"DATA\")\n",
    "SAVE_DIR = Path(\"final_models\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"--- Final Training Setup ---\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Backbone: ResNet50\")\n",
    "print(f\"Augmentations: ON (Flip, Color, Rotate +/-5, Blur)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546747e2",
   "metadata": {},
   "source": [
    "# 1. Smart Augmented Dataset\n",
    " ××—×œ×§×” ×©×˜×•×¢× ×ª ××ª ×”×“××˜×” ×•××¤×¢×™×œ×” ×¢×œ×™×• ×©×™× ×•×™×™× ×¨× ×“×•××œ×™×™× ×‘×–××Ÿ ×××ª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c832e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedRoadDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', target_size=(512, 256)):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        self.img_dir = self.root_dir / \"images\" / split\n",
    "        self.mask_dir = self.root_dir / \"masks\" / split\n",
    "        self.flow_dir = self.root_dir / \"flow\" / split\n",
    "        \n",
    "        self.images = sorted(list(self.img_dir.glob(\"*.jpg\")))\n",
    "        \n",
    "        # ××•×’×× ×˜×¦×™×•×ª ×œ×¦×‘×¢ ×‘×œ×‘×“ (×œ× ××©×¤×™×¢ ×¢×œ ×’×™××•××˜×¨×™×”)\n",
    "        self.color_jit = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
    "        self.blur = transforms.GaussianBlur(kernel_size=3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. ×˜×¢×™× ×”\n",
    "        stem = self.images[idx].stem\n",
    "        img = cv2.imread(str(self.images[idx]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(self.mask_dir / f\"{stem}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "        flow = np.load(str(self.flow_dir / f\"{stem}.npy\")) # (H, W, 2)\n",
    "\n",
    "        # 2. Resize ××—×™×“ ×œ×›×•×œ×\n",
    "        img = cv2.resize(img, self.target_size)\n",
    "        mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
    "        flow = cv2.resize(flow, self.target_size)\n",
    "\n",
    "        # ×”××¨×” ×œ-PIL/Tensor ×œ×¦×•×¨×š ××•×’×× ×˜×¦×™×•×ª ×©×œ Torchvision\n",
    "        img_pil = TF.to_pil_image(img)\n",
    "        mask_pil = TF.to_pil_image(mask)\n",
    "        flow_tensor = torch.from_numpy(flow).permute(2, 0, 1) # (2, H, W)\n",
    "\n",
    "        # --- 3. ××•×’×× ×˜×¦×™×•×ª (×¨×§ ×œ-TRAIN) ---\n",
    "        if self.split == 'train':\n",
    "            # A. ×©×™× ×•×™×™ ×¦×‘×¢ ×•×˜×©×˜×•×© (×¨×§ ×¢×œ ×”×ª××•× ×”)\n",
    "            if random.random() > 0.5:\n",
    "                img_pil = self.color_jit(img_pil)\n",
    "            if random.random() > 0.8: # ×œ×¢×™×ª×™× ×¨×—×•×§×•×ª ×™×•×ª×¨\n",
    "                img_pil = self.blur(img_pil)\n",
    "\n",
    "            # B. ×”×™×¤×•×š ××•×¤×§×™ (Horizontal Flip) - ×“×•×¨×© ×˜×™×¤×•×œ ××™×•×—×“\n",
    "            if random.random() > 0.5:\n",
    "                img_pil = TF.hflip(img_pil)\n",
    "                mask_pil = TF.hflip(mask_pil)\n",
    "                flow_tensor = TF.hflip(flow_tensor)\n",
    "                # ×”×§×¡×: ×× ×”×•×¤×›×™× ×ª××•× ×”, ×•×§×˜×•×¨ ×™××™× ×” ×”×•×¤×š ×œ×•×§×˜×•×¨ ×©×××œ×”\n",
    "                # ×œ×›×Ÿ ×—×™×™×‘×™× ×œ×”×¤×•×š ××ª ×”×¡×™××Ÿ ×©×œ ×¨×›×™×‘ ×”-X (×¢×¨×•×¥ 0)\n",
    "                flow_tensor[0, :, :] = -flow_tensor[0, :, :]\n",
    "\n",
    "            # C. ×¡×™×‘×•×‘ ×§×œ (Rotation +/- 5 degrees)\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.uniform(-5, 5)\n",
    "                # ××¡×•×‘×‘×™× ××ª ×›×•×œ× ×‘××•×ª×” ×–×•×•×™×ª ×‘×“×™×•×§\n",
    "                img_pil = TF.rotate(img_pil, angle)\n",
    "                mask_pil = TF.rotate(mask_pil, angle)\n",
    "                flow_tensor = TF.rotate(flow_tensor, angle)\n",
    "                # ×”×¢×¨×”: ×‘×¡×™×‘×•×‘ ×§×˜×Ÿ ×× ×—× ×• ××–× ×™×—×™× ××ª ×¡×™×‘×•×‘ ×¢×¨×›×™ ×”×•×§×˜×•×¨ ×¢×¦××\n",
    "\n",
    "        # 4. ×”××¨×” ×¡×•×¤×™×ª ×œ-Tensor ×•× ×¨××•×œ\n",
    "        img_tensor = TF.to_tensor(img_pil) # ×× ×¨××œ ××•×˜×•××˜×™×ª ×œ-0-1\n",
    "        mask_tensor = TF.to_tensor(mask_pil)\n",
    "        \n",
    "        # ×”-Flow ×›×‘×¨ ×‘×˜× ×–×•×¨, ×¨×§ ×œ×•×•×“× ×©×”×•× Float\n",
    "        flow_tensor = flow_tensor.float()\n",
    "\n",
    "        return img_tensor, mask_tensor, flow_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85d461",
   "metadata": {},
   "source": [
    "# 2. ResNet50 Dual-Head Model\n",
    " ×”××•×“×œ ×”×’×“×•×œ ×•×”×—×–×§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63eae772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualHeadResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ×˜×•×¢× ×™× ResNet50 ×××•××Ÿ ××¨××©\n",
    "        base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        layers = list(base.children())\n",
    "        \n",
    "        # Encoder\n",
    "        self.layer0 = nn.Sequential(*layers[:3])\n",
    "        self.layer1 = nn.Sequential(*layers[3:5])\n",
    "        self.layer2 = layers[5]\n",
    "        self.layer3 = layers[6]\n",
    "        self.layer4 = layers[7]\n",
    "        \n",
    "        # Decoder (××•×ª×× ×œ×¢×¨×•×¦×™× ×©×œ ResNet50: 2048, 1024, 512, 256)\n",
    "        self.up4 = self._up_block(2048, 1024)\n",
    "        self.up3 = self._up_block(1024 + 1024, 512)\n",
    "        self.up2 = self._up_block(512 + 512, 256)\n",
    "        self.up1 = self._up_block(256 + 256, 64)\n",
    "        self.up0 = self._up_block(64 + 64, 32)\n",
    "        \n",
    "        # Heads\n",
    "        self.potential_head = nn.Sequential(nn.Conv2d(32, 1, 1), nn.Sigmoid())\n",
    "        self.flow_head = nn.Sequential(nn.Conv2d(32, 2, 1), nn.Tanh())\n",
    "\n",
    "    def _up_block(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 2, 2),\n",
    "            nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_c), nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.layer0(x)\n",
    "        x1 = self.layer1(x0)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        \n",
    "        up4 = self.up4(x4)\n",
    "        up3 = self.up3(torch.cat([up4, x3], 1))\n",
    "        up2 = self.up2(torch.cat([up3, x2], 1))\n",
    "        up1 = self.up1(torch.cat([up2, x1], 1))\n",
    "        feat = self.up0(torch.cat([up1, x0], 1))\n",
    "        \n",
    "        return self.potential_head(feat), self.flow_head(feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f2557",
   "metadata": {},
   "source": [
    "#  3. Training Engine\n",
    " ×œ×•×œ××ª ×”××™××•×Ÿ ×”××œ××” ×¢× ×©××™×¨×ª ××“×“×™×."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8192b652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images: 6906 | Val Images: 979\n",
      "\n",
      "--- Epoch 1/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3814\n",
      "Val Loss:   0.2434 | IoU: 0.776 | Prec: 0.812 | Rec: 0.944 | EPE: 0.131\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2127\n",
      "Val Loss:   0.2096 | IoU: 0.775 | Prec: 0.932 | Rec: 0.817 | EPE: 0.111\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1951\n",
      "Val Loss:   0.2041 | IoU: 0.792 | Prec: 0.923 | Rec: 0.845 | EPE: 0.104\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1908\n",
      "Val Loss:   0.2004 | IoU: 0.823 | Prec: 0.909 | Rec: 0.894 | EPE: 0.102\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1888\n",
      "Val Loss:   0.2006 | IoU: 0.822 | Prec: 0.886 | Rec: 0.917 | EPE: 0.100\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1873\n",
      "Val Loss:   0.2003 | IoU: 0.781 | Prec: 0.936 | Rec: 0.821 | EPE: 0.098\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1861\n",
      "Val Loss:   0.1988 | IoU: 0.812 | Prec: 0.924 | Rec: 0.867 | EPE: 0.092\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1855\n",
      "Val Loss:   0.2002 | IoU: 0.801 | Prec: 0.917 | Rec: 0.860 | EPE: 0.100\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1846\n",
      "Val Loss:   0.1980 | IoU: 0.829 | Prec: 0.883 | Rec: 0.929 | EPE: 0.092\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1841\n",
      "Val Loss:   0.1984 | IoU: 0.825 | Prec: 0.912 | Rec: 0.893 | EPE: 0.092\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1834\n",
      "Val Loss:   0.1984 | IoU: 0.816 | Prec: 0.917 | Rec: 0.877 | EPE: 0.095\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1835\n",
      "Val Loss:   0.1977 | IoU: 0.820 | Prec: 0.921 | Rec: 0.879 | EPE: 0.090\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1831\n",
      "Val Loss:   0.1976 | IoU: 0.836 | Prec: 0.888 | Rec: 0.932 | EPE: 0.087\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1822\n",
      "Val Loss:   0.1978 | IoU: 0.835 | Prec: 0.891 | Rec: 0.926 | EPE: 0.091\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1822\n",
      "Val Loss:   0.1978 | IoU: 0.833 | Prec: 0.910 | Rec: 0.905 | EPE: 0.087\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1816\n",
      "Val Loss:   0.1987 | IoU: 0.827 | Prec: 0.888 | Rec: 0.921 | EPE: 0.089\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1811\n",
      "Val Loss:   0.1981 | IoU: 0.832 | Prec: 0.894 | Rec: 0.920 | EPE: 0.090\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "LR: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1794\n",
      "Val Loss:   0.1975 | IoU: 0.834 | Prec: 0.880 | Rec: 0.938 | EPE: 0.084\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "LR: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1783\n",
      "Val Loss:   0.1966 | IoU: 0.841 | Prec: 0.910 | Rec: 0.914 | EPE: 0.082\n",
      ">>> ğŸ’¾ New Best Model Saved!\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "LR: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1776\n",
      "Val Loss:   0.1972 | IoU: 0.829 | Prec: 0.913 | Rec: 0.897 | EPE: 0.087\n",
      "\n",
      "âœ… Final Training Complete. Model saved in 'final_models/'.\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(pred_pot, target_pot, pred_flow, target_flow):\n",
    "    # Segmentation Metrics (IoU, Precision, Recall)\n",
    "    pred_bin = (pred_pot > 0.5).float()\n",
    "    target_bin = (target_pot > 0.5).float()\n",
    "    \n",
    "    tp = (pred_bin * target_bin).sum().item()\n",
    "    fp = (pred_bin * (1-target_bin)).sum().item()\n",
    "    fn = ((1-pred_bin) * target_bin).sum().item()\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    iou = tp / (tp + fp + fn + epsilon)\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    \n",
    "    # Flow Metrics (EPE)\n",
    "    diff = pred_flow - target_flow\n",
    "    epe = torch.norm(diff, dim=1).mean().item()\n",
    "    \n",
    "    return iou, precision, recall, epe\n",
    "\n",
    "def train_model():\n",
    "    # 1. Data Loaders (Full Dataset)\n",
    "    train_ds = AugmentedRoadDataset(DATA_DIR, 'train')\n",
    "    val_ds = AugmentedRoadDataset(DATA_DIR, 'val')\n",
    "    \n",
    "    # num_workers=2 ×××™×¥ ××ª ×”×˜×¢×™× ×” (×‘×–×”×™×¨×•×ª ×¢× ×•×•×™× ×“×•×¡)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Train Images: {len(train_ds)} | Val Images: {len(val_ds)}\")\n",
    "    \n",
    "    # 2. Model Setup\n",
    "    model = DualHeadResNet50().to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # 3. Losses (The Winners)\n",
    "    criterion_pot = nn.BCELoss() # Best for Map\n",
    "    criterion_flow = nn.MSELoss() # Best for Vector Length & Direction combined\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = []\n",
    "    \n",
    "    # 4. Main Loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"LR: {current_lr}\")\n",
    "        \n",
    "        # Train Step\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        for imgs, masks, flows in loop:\n",
    "            imgs, masks, flows = imgs.to(DEVICE), masks.to(DEVICE), flows.to(DEVICE)\n",
    "            \n",
    "            # Forward\n",
    "            pred_pot, pred_flow = model(imgs)\n",
    "            \n",
    "            # Loss\n",
    "            loss_pot = criterion_pot(pred_pot, masks)\n",
    "            loss_flow = criterion_flow(pred_flow, flows)\n",
    "            loss = loss_pot + loss_flow\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        total_iou = 0\n",
    "        total_prec = 0\n",
    "        total_rec = 0\n",
    "        total_epe = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, masks, flows in val_loader:\n",
    "                imgs, masks, flows = imgs.to(DEVICE), masks.to(DEVICE), flows.to(DEVICE)\n",
    "                pred_pot, pred_flow = model(imgs)\n",
    "                \n",
    "                # Loss\n",
    "                loss_pot = criterion_pot(pred_pot, masks)\n",
    "                loss_flow = criterion_flow(pred_flow, flows)\n",
    "                val_loss += (loss_pot + loss_flow).item()\n",
    "                \n",
    "                # Metrics\n",
    "                iou, prec, rec, epe = calculate_metrics(pred_pot, masks, pred_flow, flows)\n",
    "                total_iou += iou\n",
    "                total_prec += prec\n",
    "                total_rec += rec\n",
    "                total_epe += epe\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_iou = total_iou / len(val_loader)\n",
    "        avg_prec = total_prec / len(val_loader)\n",
    "        avg_rec = total_rec / len(val_loader)\n",
    "        avg_epe = total_epe / len(val_loader)\n",
    "        \n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Val Loss:   {avg_val_loss:.4f} | IoU: {avg_iou:.3f} | Prec: {avg_prec:.3f} | Rec: {avg_rec:.3f} | EPE: {avg_epe:.3f}\")\n",
    "        \n",
    "        # Scheduler Step\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save Best\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), SAVE_DIR / \"final_model_r50_best.pth\")\n",
    "            print(\">>> ğŸ’¾ New Best Model Saved!\")\n",
    "            \n",
    "        # Log History\n",
    "        history.append({\n",
    "            \"Epoch\": epoch+1,\n",
    "            \"Train Loss\": avg_train_loss,\n",
    "            \"Val Loss\": avg_val_loss,\n",
    "            \"IoU\": avg_iou,\n",
    "            \"Precision\": avg_prec,\n",
    "            \"Recall\": avg_rec,\n",
    "            \"EPE\": avg_epe,\n",
    "            \"LR\": current_lr\n",
    "        })\n",
    "        \n",
    "    # Save Final Report\n",
    "    pd.DataFrame(history).to_csv(SAVE_DIR / \"training_history.csv\", index=False)\n",
    "    print(\"\\nâœ… Final Training Complete. Model saved in 'final_models/'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Starting Extended Training (Fine-Tuning) on cuda\n",
      "   Target LR: 1e-05 | Epochs: 20\n",
      "   Loading weights from: final_models\\final_model_r50_best.pth\n",
      "\n",
      "--- Extended Epoch 1/20 ---\n",
      "LR: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1766\n",
      "Val Loss:   0.1974 | IoU: 0.836 | Prec: 0.905 | Rec: 0.912 | EPE: 0.086\n",
      ">>> ğŸ’¾ Extended Best Model Saved!\n",
      "\n",
      "--- Extended Epoch 2/20 ---\n",
      "LR: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1257/1727 [07:15<02:52,  2.72it/s, loss=0.12] "
     ]
    }
   ],
   "source": [
    "# --- ×”×’×“×¨×•×ª ×œ×”××©×š ××™××•×Ÿ (Fine-Tuning) ---\n",
    "EXTENDED_EPOCHS = 20\n",
    "FINE_TUNE_LR = 1e-5  # ×§×¦×‘ ×œ××™×“×” ×¢×“×™×Ÿ ×××•×“ (×¤×™ 10 ×¤×—×•×ª ××”××§×•×¨×™)\n",
    "\n",
    "def train_extended_session():\n",
    "    print(f\"\\nğŸš€ Starting Extended Training (Fine-Tuning) on {DEVICE}\")\n",
    "    print(f\"   Target LR: {FINE_TUNE_LR} | Epochs: {EXTENDED_EPOCHS}\")\n",
    "\n",
    "    # 1. ×˜×¢×™× ×ª ×”×“××˜×” (×©×™××•×© ×‘××—×œ×§×•×ª ×”×§×™×™××•×ª)\n",
    "    train_ds = AugmentedRoadDataset(DATA_DIR, 'train')\n",
    "    val_ds = AugmentedRoadDataset(DATA_DIR, 'val')\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 2. ×™×¦×™×¨×ª ×”××•×“×œ ×•×˜×¢×™× ×ª ×”××©×§×•×œ×•×ª ×”×›×™ ×˜×•×‘×•×ª ××”×¨×™×¦×” ×”×§×•×“××ª\n",
    "    model = DualHeadResNet50().to(DEVICE)\n",
    "    best_model_path = SAVE_DIR / \"final_model_r50_best.pth\"\n",
    "    \n",
    "    if best_model_path.exists():\n",
    "        print(f\"   Loading weights from: {best_model_path}\")\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    else:\n",
    "        print(f\"âŒ Error: {best_model_path} not found! Run the main training first.\")\n",
    "        return\n",
    "\n",
    "    # 3. ×”×’×“×¨×ª ××•×¤×˜×™××™×™×–×¨ ×¢×“×™×Ÿ\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=FINE_TUNE_LR, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    criterion_pot = nn.BCELoss()\n",
    "    criterion_flow = nn.MSELoss()\n",
    "    \n",
    "    # ×××¤×¡×™× ××ª ×”-Best Loss ×›×“×™ ×œ×©××•×¨ ×©×™×¤×•×¨×™× ××§×•××™×™× ×©×œ ×”×¡×©×Ÿ ×”×–×”\n",
    "    best_val_loss = float('inf')\n",
    "    history = []\n",
    "\n",
    "    # 4. ×œ×•×œ××ª ×”××™××•×Ÿ ×”× ×•×¡×¤×ª\n",
    "    for epoch in range(EXTENDED_EPOCHS):\n",
    "        print(f\"\\n--- Extended Epoch {epoch+1}/{EXTENDED_EPOCHS} ---\")\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']}\")\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        loop = tqdm(train_loader, desc=\"Fine-Tuning\", leave=False)\n",
    "        \n",
    "        for imgs, masks, flows in loop:\n",
    "            imgs, masks, flows = imgs.to(DEVICE), masks.to(DEVICE), flows.to(DEVICE)\n",
    "            \n",
    "            pot_out, flow_out = model(imgs)\n",
    "            loss = criterion_pot(pot_out, masks) + criterion_flow(flow_out, flows)\n",
    "            \n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        total_iou = 0\n",
    "        total_prec = 0\n",
    "        total_rec = 0\n",
    "        total_epe = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, masks, flows in val_loader:\n",
    "                imgs, masks, flows = imgs.to(DEVICE), masks.to(DEVICE), flows.to(DEVICE)\n",
    "                pot_out, flow_out = model(imgs)\n",
    "                \n",
    "                val_loss += (criterion_pot(pot_out, masks) + criterion_flow(flow_out, flows)).item()\n",
    "                iou, prec, rec, epe = calculate_metrics(pot_out, masks, flow_out, flows)\n",
    "                \n",
    "                total_iou += iou\n",
    "                total_prec += prec\n",
    "                total_rec += rec\n",
    "                total_epe += epe\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        metrics = [\n",
    "            total_iou / len(val_loader),\n",
    "            total_prec / len(val_loader),\n",
    "            total_rec / len(val_loader),\n",
    "            total_epe / len(val_loader)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Val Loss:   {avg_val_loss:.4f} | IoU: {metrics[0]:.3f} | Prec: {metrics[1]:.3f} | Rec: {metrics[2]:.3f} | EPE: {metrics[3]:.3f}\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # ×©×•××¨×™× ×‘×©× ×—×“×© (_extended)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), SAVE_DIR / \"final_model_r50_extended.pth\")\n",
    "            print(\">>> ğŸ’¾ Extended Best Model Saved!\")\n",
    "            \n",
    "        history.append({\n",
    "            \"Epoch\": f\"Ext-{epoch+1}\",\n",
    "            \"Train Loss\": avg_train_loss,\n",
    "            \"Val Loss\": avg_val_loss,\n",
    "            \"IoU\": metrics[0],\n",
    "            \"EPE\": metrics[3]\n",
    "        })\n",
    "\n",
    "    # ×©×•××¨×™× ××ª ×”×”×™×¡×˜×•×¨×™×” ×œ×§×•×‘×¥ × ×¤×¨×“\n",
    "    pd.DataFrame(history).to_csv(SAVE_DIR / \"training_history_extended.csv\", index=False)\n",
    "    print(\"\\nâœ… Fine-Tuning Complete. Model saved as 'final_model_r50_extended.pth'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_extended_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40b883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sdxl)",
   "language": "python",
   "name": "sdxl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
